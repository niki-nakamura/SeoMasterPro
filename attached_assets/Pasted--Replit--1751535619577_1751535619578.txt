以下は **残タスクの全体像**、**あなたのローカル環境で実行する作業**、そして **Replit に送る具体的な指示文** をまとめたものです。最初にいちばん重要なポイントを一段落で要約し、その後に詳細を段階的に整理しています。

## 要約（まずこれだけ把握）

現時点で足りないのは **① 既存記事のベクトル埋め込み生成と投入、② ローカル LLM（Ollama）─バックエンド間の動作確認、③ 大量処理時のパフォーマンス・UX 改善** の３点です。Embed 生成はローカル PC で `tsx scripts/embed_existing_articles.ts` を１回走らせれば完了しますが、その前に **Ollama に `mxbai‑embed‑large` モデルを pull→起動** し、**pgvector に HNSW インデックスを追加** しておくと検索速度が向上します。ローカルで確認したあと、Replit 環境には **(1) 新インデックス用マイグレーション適用・(2) フロント UI の進捗バー／プレビュー実装用ブランチを pull & deploy** という２系統の指示を送れば OK です。以下、どこで何を実行するかを具体的に示します。

---

## 残りの TODO 一覧と実行場所

| #  | 項目                                 | 実行場所                   | ゴール／備考                                                                                                                  |
| -- | ---------------------------------- | ---------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| 1  | `mxbai-embed-large` モデルの取得         | **ローカル**               | `ollama pull mxbai-embed-large` で 1536 次元 embed ([ollama.com][1])                                                       |
| 2  | Ollama サーバー起動                      | **ローカル**               | `ollama serve -p 11434`（デフォルト）; embed API を有効化 ([ollama.com][2])                                                        |
| 3  | 既存記事のベクトル生成                        | **ローカル**               | `tsx scripts/embed_existing_articles.ts` で 3 記事の vector を `content_vectors` に Insert ([tsx.is][3], [huggingface.co][4]) |
| 4  | pgvector HNSW インデックス作成             | **Replit / Neon DB**   | `CREATE INDEX ON content_vectors USING hnsw (embedding vector_cosine_ops);` ([neon.com][5], [neon.com][6])              |
| 5  | `/api/llm-context` エンドポイント E2E テスト | **ローカル → Replit**      | Postman 等で Top‑k≦3 が返るか確認                                                                                               |
| 6  | 大量記事処理ベンチマーク                       | **ローカル**               | 10→100 記事を擬似投入し秒/req を計測                                                                                                |
| 7  | スクレイピングレート・UA ランダマイズ調整             | **ローカル**               | Playwright に動的 UA ミドルウェア追加（例実装 below） ([zenrows.com][7], [scrapeops.io][8])                                             |
| 8  | 進捗バー & プレビュー UI                    | **Replit (Front)**     | React state machine でステップ毎に progress 更新                                                                                 |
| 9  | 記事編集／カスタマイズ機能（次フェーズ）               | **Replit (Front+API)** | PUT `/api/articles/:id` を追加                                                                                             |
| 10 | ユーザー品質評価＋フィードバック収集                 | **Replit**             | テーブル `user_feedback` & simple form                                                                                      |

---

## あなたが **手元（ローカル PC）** で実行する作業

1. **Ollama セットアップ**

   ```bash
   # 1) embed‑model を取得
   ollama pull mxbai-embed-large          # 初回のみ数分
   # 2) サーバーを起動
   ollama serve -p 11434 &
   ```

   *`mxbai‑embed‑large` は 1536‑d で OpenAI text‑embedding‑3‑large 並みの精度を確認 ([huggingface.co][4], [ollama.com][1])。*

2. **環境変数をローカル `.env` に追記**

   ```env
   OLLAMA_HOST=http://localhost:11434
   ```

3. **ベクトル生成スクリプトの実行**

   ```bash
   pnpm i           # 依存の再確認
   pnpm tsx scripts/embed_existing_articles.ts
   ```

   *`tsx` は TypeScript ファイルをトランスパイル無しで即時実行する CLI ([tsx.is][3])。*

4. **Neon DB に Remote で接続し、生成ベクトルを確認**

   ```sql
   SELECT count(*) FROM content_vectors;
   ```

5. **E2E テスト**

   ```bash
   # 例: curl
   curl -X POST https://seo-master-pro-nikinakamu.replit.app/api/llm-context \
        -H "Content-Type: application/json" \
        -d '{"article_id": "xxx", "top_k": 3}'
   ```

   正常に類似記事が返れば OK。

---

## **Replit** へ送る指示文（コピーして Slack / Issue に貼り付け）

> ### 🛠️ 指示：ベクトル検索最終セットアップ & UI 改善
>
> 1. **DB マイグレーション**
>
>    ```sql
>    -- Enable pgvector extension if not yet
>    CREATE EXTENSION IF NOT EXISTS vector;
>    -- 高速化のため HNSW インデックスを追加
>    CREATE INDEX IF NOT EXISTS content_vectors_hnsw
>      ON content_vectors
>      USING hnsw (embedding vector_cosine_ops);
>    ```
> 2. **バックエンド**
>
>    * `content_vectors` テーブルにローカル生成済み 1536‑d ベクトルが Insert 済み。
>    * `/api/llm-context` が Index 使用時に Seq Scan になっていないか EXPLAIN で確認。
> 3. **フロントエンド**
>
>    * `feature/progress-preview` ブランチを pull → deploy。
>    * Wizard 画面に **進捗バー**（5 アイコン）と **生成プレビュー**（read‑only textarea）を表示。ステップ更新は `articles.status` を watch し、Tailwind `animate-[progress]` を利用。
> 4. **CI/CD**
>
>    * 上記ブランチマージ後、自動デプロイを `autoscale: true` のまま実行し、URL を共有してください。
> 5. **パフォーマンス確認**
>
>    * `/api/scrape` を 10 並列で叩き、平均応答 <4 s を目標。慢性 429 が出たら `SCRAPE_DELAY_MS` を 5000 へ調整。

---

## スクリプト／コマンド実行場所の解説

| コマンド例                                    | 実行するホスト                      | 理由                            |
| ---------------------------------------- | ---------------------------- | ----------------------------- |
| `ollama pull …` / `ollama serve`         | **ローカル**                     | Replit コンテナでは Ollama 常駐が難しいため |
| `tsx scripts/embed_existing_articles.ts` | **ローカル**                     | 上記 Ollama API を叩いて embed を生成  |
| `CREATE INDEX … USING hnsw …`            | **Neon / Replit DB Console** | インデックスは DB サーバー側に作成           |
| `pnpm run dev` (フロント)                    | **Replit**                   | 本番ホットリロードを確認                  |
| `curl /api/llm-context`                  | **ローカル or Replit Shell**     | ネット経由で E2E を検証                |

---

## 参考実装スニペット

```ts
// playwright/ua-middleware.ts
import { chromium, BrowserContext } from 'playwright';
import randomUa from 'random-useragent';  // npm i random-useragent

export async function newContext(): Promise<BrowserContext> {
  return chromium.launchPersistentContext('', {
    userAgent: randomUa.getRandom(),  // UA を毎回変更
  });
}
```

*ランダム UA は anti‑bot 回避に有効 ([zenrows.com][7], [scrapeops.io][8])。*

---

## 追加で参照した主要情報源

1. Ollama 公式ブログ – Embedding Models ([ollama.com][2])
2. `mxbai-embed-large` モデルカード（Hugging Face / Ollama Library） ([huggingface.co][4], [ollama.com][1])
3. TypeScript Execute (tsx) ドキュメント ([tsx.is][3])
4. Neon Docs – pgvector 拡張と HNSW インデックス ([neon.com][5], [neon.com][6])
5. Drizzle ORM での pgvector 利用ガイド ([orm.drizzle.team][9])
6. Playwright × Bot 回避 Tips（ZenRows & ScrapeOps） ([zenrows.com][7], [scrapeops.io][8])
7. Supabase OAuth & Next.js 事例（参考） ([reddit.com][10])

これで Phase 1 の作業はすべて網羅されました。ご不明点があればいつでもどうぞ。

[1]: https://ollama.com/library/mxbai-embed-large?utm_source=chatgpt.com "mxbai-embed-large - Ollama"
[2]: https://ollama.com/blog/embedding-models?utm_source=chatgpt.com "Embedding models · Ollama Blog"
[3]: https://tsx.is/?utm_source=chatgpt.com "TypeScript Execute (tsx) | tsx"
[4]: https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1?utm_source=chatgpt.com "mixedbread-ai/mxbai-embed-large-v1 - Hugging Face"
[5]: https://neon.com/docs/extensions/pgvector?utm_source=chatgpt.com "The pgvector extension - Neon Docs"
[6]: https://neon.com/blog/optimizing-vector-search-performance-with-pgvector?utm_source=chatgpt.com "Optimizing vector search performance with pgvector - Neon"
[7]: https://www.zenrows.com/blog/avoid-playwright-bot-detection?utm_source=chatgpt.com "How to Avoid Bot Detection with Playwright - ZenRows"
[8]: https://scrapeops.io/playwright-web-scraping-playbook/nodejs-playwright-using-fake-user-agents/?utm_source=chatgpt.com "Playwright Guide - Using Fake User Agents - ScrapeOps"
[9]: https://orm.drizzle.team/docs/guides/vector-similarity-search?utm_source=chatgpt.com "Vector similarity search with pgvector extension - Drizzle ORM"
[10]: https://www.reddit.com/r/Supabase/comments/18x0zz1/nextjs_supabase_example_project_for_oauth/?utm_source=chatgpt.com "Next.js + Supabase Example Project for OAuth Authentication - Reddit"
