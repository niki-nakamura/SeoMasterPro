#!/usr/bin/env node
/**
 * Deployment preparation script for WebLLM + Ollama dual-mode system
 * This script helps prepare the final deployment to Replit
 */

import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  magenta: '\x1b[35m',
  cyan: '\x1b[36m'
};

function log(color, message) {
  console.log(`${color}${message}${colors.reset}`);
}

function checkFile(filePath, description) {
  const fullPath = path.join(__dirname, '..', filePath);
  if (fs.existsSync(fullPath)) {
    log(colors.green, `‚úÖ ${description}: ${filePath}`);
    return true;
  } else {
    log(colors.red, `‚ùå ${description}: ${filePath} (Missing)`);
    return false;
  }
}

function checkEnvironmentVariable(varName, description) {
  const value = process.env[varName];
  if (value && value !== 'undefined') {
    log(colors.green, `‚úÖ ${description}: ${varName}=${value}`);
    return true;
  } else {
    log(colors.yellow, `‚ö†Ô∏è  ${description}: ${varName} (Not set)`);
    return false;
  }
}

async function prepareDeployment() {
  log(colors.bright, 'üöÄ WebLLM Deployment Preparation Checklist\n');
  
  let allReady = true;
  
  // Check essential files
  log(colors.cyan, 'üìÅ Checking essential files...');
  allReady &= checkFile('public/webllm-models/llama3-8b/model.json', 'WebLLM model config');
  allReady &= checkFile('client/src/lib/webllm.ts', 'WebLLM library');
  allReady &= checkFile('client/src/pages/settings.tsx', 'Settings page');
  allReady &= checkFile('client/src/pages/chat.tsx', 'Chat page');
  allReady &= checkFile('server/services/ollama-manager.ts', 'Ollama manager');
  
  console.log();
  
  // Check model.json configuration
  log(colors.cyan, '‚öôÔ∏è  Checking WebLLM model configuration...');
  try {
    const modelJsonPath = path.join(__dirname, '..', 'public/webllm-models/llama3-8b/model.json');
    const modelConfig = JSON.parse(fs.readFileSync(modelJsonPath, 'utf8'));
    
    if (modelConfig.model_url.includes('cdn.example.com')) {
      log(colors.yellow, '‚ö†Ô∏è  model.json still uses example CDN URL');
      log(colors.blue, '   ‚Üí You need to update this to your actual CDN URL');
      allReady = false;
    } else {
      log(colors.green, `‚úÖ Model URL configured: ${modelConfig.model_url}`);
    }
    
    log(colors.blue, `   Model ID: ${modelConfig.model_id}`);
    log(colors.blue, `   VRAM Required: ${modelConfig.vram_required_MB} MB`);
  } catch (error) {
    log(colors.red, `‚ùå Failed to read model.json: ${error.message}`);
    allReady = false;
  }
  
  console.log();
  
  // Check environment variables
  log(colors.cyan, 'üîß Checking environment variables...');
  checkEnvironmentVariable('DATABASE_URL', 'Database connection');
  checkEnvironmentVariable('LITE_MODE', 'Lite mode setting');
  
  // Check deployment-specific variables
  const hasModelUrl = checkEnvironmentVariable('VITE_MODEL_URL', 'WebLLM model URL (client)');
  if (!hasModelUrl) {
    log(colors.blue, '   ‚Üí Set this in Replit Secrets for custom CDN URL');
  }
  
  console.log();
  
  // Print deployment instructions
  log(colors.magenta, 'üìã Final Deployment Steps:');
  console.log();
  
  log(colors.bright, '1. üì¶ Upload Model Files to CDN');
  log(colors.blue, '   ‚Ä¢ Upload llama3-8b-q4f16_1-MLC/ folder to Cloudflare R2');
  log(colors.blue, '   ‚Ä¢ Configure CORS: {"AllowedOrigins":["*"],"AllowedMethods":["GET","HEAD"]}');
  log(colors.blue, '   ‚Ä¢ Note your CDN URL: https://<ACCOUNT_ID>.r2.cloudflarestorage.com/');
  
  console.log();
  
  log(colors.bright, '2. üîó Update Model URL');
  log(colors.blue, '   ‚Ä¢ Edit public/webllm-models/llama3-8b/model.json');
  log(colors.blue, '   ‚Ä¢ Change "model_url" to your actual CDN URL');
  log(colors.blue, '   ‚Ä¢ Commit the changes');
  
  console.log();
  
  log(colors.bright, '3. üîë Set Replit Secrets');
  log(colors.blue, '   ‚Ä¢ Go to Replit ‚Üí Secrets tab');
  log(colors.blue, '   ‚Ä¢ Add: VITE_MODEL_URL=https://your-account.r2.cloudflarestorage.com/llama3-8b-q4f16_1-MLC/');
  log(colors.blue, '   ‚Ä¢ (Optional) Add other secrets like VITE_SUPABASE_URL');
  
  console.log();
  
  log(colors.bright, '4. üöÄ Deploy to Production');
  log(colors.blue, '   ‚Ä¢ Click Deploy ‚Üí New deployment in Replit');
  log(colors.blue, '   ‚Ä¢ Wait for deployment to complete');
  
  console.log();
  
  log(colors.bright, '5. ‚úÖ Test Deployment');
  log(colors.blue, '   ‚Ä¢ Chrome/Edge: Navigate to /settings ‚Üí Click "„Çµ„Éº„Éê„Éº„ÇíËµ∑Âãï"');
  log(colors.blue, '   ‚Ä¢ Should show progress: "Fetching params 0 / 1900 MB"');
  log(colors.blue, '   ‚Ä¢ Test chat at /chat ‚Üí Send "Hello" message');
  log(colors.blue, '   ‚Ä¢ Firefox: Should fallback to Ollama tinymistral');
  
  console.log();
  
  if (allReady) {
    log(colors.green, 'üéâ All checks passed! Ready for deployment.');
  } else {
    log(colors.yellow, '‚ö†Ô∏è  Some issues need to be resolved before deployment.');
  }
  
  console.log();
  log(colors.cyan, 'Expected Performance:');
  log(colors.blue, '   ‚Ä¢ WebGPU (Chrome/Edge): Llama 3 8B @ 9-11 tokens/sec');
  log(colors.blue, '   ‚Ä¢ Fallback (Firefox): tinymistral @ 6 tokens/sec');
  log(colors.blue, '   ‚Ä¢ Model download: ~1.9GB (cached after first load)');
  log(colors.blue, '   ‚Ä¢ Second load: <3 seconds (IndexedDB cache)');
  
  console.log();
  log(colors.cyan, 'Cost Estimate:');
  log(colors.blue, '   ‚Ä¢ Replit hosting: ~$1.4/month');
  log(colors.blue, '   ‚Ä¢ Cloudflare R2: ~$0.20/month (1.9GB storage)');
  log(colors.blue, '   ‚Ä¢ Total: ~$1.60/month for complete local LLM system');
}

// Run the preparation check
if (import.meta.url === `file://${process.argv[1]}`) {
  prepareDeployment().catch(console.error);
}

export { prepareDeployment };